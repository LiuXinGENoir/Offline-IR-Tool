{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IR Tool:\n",
    "\n",
    "Pre-processes text\n",
    "- Tokenisation\n",
    "- Stopword removal using this list of words. You MUST use this list.\n",
    "- Porter stemming. You can use packages for this part, such as Snowball or NLTK.\n",
    "\n",
    "Creates a positional inverted index\n",
    "    Your index can have whatever structure you like, and can be stored in any format you like, but you will need to output it to a text file using the format specfied below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import xml.dom.minidom  \n",
    "import string\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "changes the .xml to the change.txt in root,\n",
    "only changes the contant of what I need: Document Number, Headlin and text.\n",
    "'''\n",
    "def changeXML(txt_path):    \n",
    "\n",
    "    #The path to store trec.sample.xml, change it as needed \n",
    "    #e.g. txt_path='D://ATFC//TTDS//Sem1//CW1//collections//trec.sample.xml'\n",
    "    Txt='change.txt'\n",
    "    DOMTree = xml.dom.minidom.parse(txt_path)\n",
    "    annotation = DOMTree.documentElement  \n",
    "    objects = annotation.getElementsByTagName(\"DOC\")  \n",
    "    f = open(Txt, \"w\")\n",
    "\n",
    "    for object in objects:  \n",
    "        a1 = object.getElementsByTagName(\"DOCNO\")[0]\n",
    "        if a1.childNodes == []:\n",
    "                b1 = ''\n",
    "        else:\n",
    "            b1 = a1.childNodes[0].data\n",
    "    \n",
    "        a2 = object.getElementsByTagName(\"HEADLINE\")[0]\n",
    "        if a2.childNodes == []:\n",
    "            b2 = ''\n",
    "        else:\n",
    "            b2 = a2.childNodes[0].data\n",
    "            \n",
    "        a3 = object.getElementsByTagName(\"TEXT\")[0]\n",
    "        if a3.childNodes == []:\n",
    "            b3 = ''\n",
    "        else:\n",
    "            b3 = a3.childNodes[0].data\n",
    "              \n",
    "        contents = 'ID: '+b1 +'\\nHeadline '+ b2 + b3\n",
    "        f.write(contents)\n",
    "    f.close()\n",
    "\n",
    "        \n",
    "def tokenisation():\n",
    "    stri = open('change.txt').read()\n",
    "    for i in stri:\n",
    "        if i in string.punctuation:\n",
    "            stri = stri.replace(i,\" \")\n",
    "    with open('change.txt','w') as f:\n",
    "        f.write(\"{}\\n\".format(stri))\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "def stopwordANDstemming(stop_word_path):\n",
    "    #The path to store englishST.txt, change it as needed  \n",
    "    porter_stemmer = PorterStemmer()\n",
    "    afterstri = []\n",
    "    global stop_words\n",
    "    global word_list\n",
    "    sto = open(stop_word_path).read()\n",
    "    stop_words = set(sto)\n",
    "    \n",
    "    with open('change.txt','r') as f :\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            lline = line.split()\n",
    "            for i in lline:\n",
    "                if i not in stop_words:\n",
    "                    i = porter_stemmer.stem(i)\n",
    "                    afterstri.append(i)\n",
    "    f.close()\n",
    "    \n",
    "    with open('change.txt','w') as fs:\n",
    "        for i in afterstri:\n",
    "            if i =='id':\n",
    "                fs.write('\\n')\n",
    "            if i == 'headlin':\n",
    "                fs.write(\" \")\n",
    "            else:\n",
    "                fs.write(\" {}\".format(i))\n",
    "    fs.close()\n",
    "    \n",
    "    word_list = list(set(afterstri))\n",
    "    word_list.sort()\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index():\n",
    "    \n",
    "    global word_list\n",
    "    global dictionary\n",
    "    context = []\n",
    "    docno = []\n",
    "    global Word_Documentno\n",
    "    global worddf\n",
    "\n",
    "    with open('change.txt','r') as fi:\n",
    "        lines = fi.readlines()\n",
    "        for line in lines:\n",
    "            lline=line.split()\n",
    "            if not len(lline):\n",
    "                continue\n",
    "            else:\n",
    "                docno.append(lline[1])\n",
    "                context.append(lline[2:])\n",
    "    fi.close()\n",
    "    dictionary = dict(zip(docno,context))\n",
    "    \n",
    "    for word in word_list:\n",
    "        df = 0\n",
    "        memol = []\n",
    "        for key in dictionary.keys():\n",
    "            if word in dictionary[key]:\n",
    "                memol.append(key)\n",
    "                df = df+1\n",
    "        Word_Documentno.append(memol)\n",
    "        worddf.append(df)\n",
    "\n",
    "    a = 0\n",
    "    with open('index.txt','w') as f:\n",
    "        for word in word_list:   \n",
    "            f.write(\"{0}:({1})\\n\".format(word, worddf[a]))\n",
    "            line = Word_Documentno[a]\n",
    "            for number in line:\n",
    "                f.write(\"\\t\\t{0}:{1}\\n\".format(number,[i for i,x in enumerate(dictionary[number]) if x ==word]))\n",
    "            a = a+1\n",
    "    f.close()   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Uses your positional inverted index to perform:\n",
    "- Boolean search\n",
    "- Phrase search\n",
    "- Proximity search\n",
    "- Ranked IR based on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search(query):\n",
    "    \n",
    "    global word_list\n",
    "    global stop_words\n",
    "    global Word_Documentno\n",
    "    porter_stemmer = PorterStemmer()\n",
    "\n",
    "    if query not in stop_words:\n",
    "        query_use= porter_stemmer.stem(query)\n",
    "        documents = Word_Documentno[word_list.index(query_use)]\n",
    "        flag = 1\n",
    "        score = [0]\n",
    "    else:\n",
    "        flag = 1\n",
    "        documents = 'This word is a stop word!'\n",
    "        score = [0]\n",
    "    return flag, documents, score\n",
    "\n",
    "    \n",
    "    \n",
    "def PhraseSearch(query, prox):\n",
    "    \n",
    "    global word_list\n",
    "    global Word_Documentno\n",
    "    global dictionary\n",
    "    \n",
    "    porter_stemmer = PorterStemmer()\n",
    "    documentS = []\n",
    "    documentSs_together = []\n",
    "    documentsS1 = []\n",
    "    documentsS2 = []\n",
    "    \n",
    "    punctuation_string = string.punctuation\n",
    "    for i in query:\n",
    "        if i in string.punctuation:\n",
    "            query = query.replace(i,\" \")\n",
    "    Q = query.split()\n",
    "    word1 = Q[0]\n",
    "    word2 = Q[1]\n",
    "    \n",
    "    word1_use= porter_stemmer.stem(word1)\n",
    "    documentsS1 = Word_Documentno[word_list.index(word1_use)]\n",
    "        \n",
    "    word2_use= porter_stemmer.stem(word2)\n",
    "    documentsS2 = Word_Documentno[word_list.index(word2_use)]\n",
    "    documentSs_together = list(set(documentsS1)|set(documentsS2))\n",
    "    \n",
    "    if len(documentSs_together)<1:\n",
    "        documentS = ['There no document include this phrase.']\n",
    "    else:\n",
    "        for number in documentSs_together:\n",
    "            list1 = [i for i,x in enumerate(dictionary[number]) if x ==word1_use]\n",
    "            list2 = [y for y,x in enumerate(dictionary[number]) if x ==word2_use]\n",
    "            for i in list1:\n",
    "                for y in list2:\n",
    "                    if abs(i-y)<=prox:\n",
    "                        documentS.append(number)\n",
    "    flag = 1\n",
    "    score = [0]\n",
    "    documentS = list(set(documentS))\n",
    "    documentS = [int(x) for x in documentS]\n",
    "    documentS.sort()\n",
    "    documentS = [str(x) for x in documentS]\n",
    "\n",
    "    return flag, documentS, score\n",
    "                    \n",
    "             \n",
    "    \n",
    "def BooleanSearch(word1, operator, word2):\n",
    "    \n",
    "    global word_list\n",
    "    global Word_Documentno\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    documents_together = []\n",
    "    documents1 = []\n",
    "    documents2 = []\n",
    "    \n",
    "    if '\"' in word1:\n",
    "        flag1, documents1, score1 = PhraseSearch(word1, prox = 1)\n",
    "    else:\n",
    "        word1_use= porter_stemmer.stem(word1)\n",
    "        documents1 = Word_Documentno[word_list.index(word1_use)]\n",
    "        \n",
    "    if  '\"' in word2:\n",
    "        flag2, documents2, score2 = PhraseSearch(word2, prox = 1)\n",
    "    else:\n",
    "        word2_use= porter_stemmer.stem(word2)\n",
    "        documents2 = Word_Documentno[word_list.index(word2_use)]\n",
    "        \n",
    "    if operator == 'AND':\n",
    "        documents_together = list(set(documents1)&set(documents2))\n",
    "    elif operator == 'OR':\n",
    "        documents_together = list(set(documents1)|set(documents2))\n",
    "    elif operator == 'AND NOT':\n",
    "        documents_together = list(set(documents1)-set(documents2))\n",
    "    else:\n",
    "        documents_together[0] = 'Ohoh...The operator has something wrong...'\n",
    "        \n",
    "    flag = 1\n",
    "    score = [0]\n",
    "    documents_together = list(set(documents_together))\n",
    "    documents_together = [int(x) for x in documents_together]\n",
    "    documents_together.sort()\n",
    "    documents_together = [str(x) for x in documents_together]\n",
    "\n",
    "    return flag, documents_together, score\n",
    "            \n",
    "               \n",
    "    \n",
    "def get_proxomity_query_elements(query):  \n",
    "    \n",
    "    sign_open = query.index('(')\n",
    "    sign_close = query.index(')')\n",
    "    proximity_number = int(query[1:sign_open])\n",
    "    q = query[sign_open + 1:sign_close]\n",
    "    \n",
    "    punctuation_string = string.punctuation\n",
    "    for i in q:\n",
    "        if i in string.punctuation:\n",
    "            q = q.replace(i,\" \")\n",
    "    Q = q.split()\n",
    "    word1 = Q[0]\n",
    "    word2 = Q[1]\n",
    "        \n",
    "    return word1, word2, proximity_number\n",
    "    \n",
    "\n",
    "    \n",
    "def Proximitysearch(word1, word2, proximity_number):\n",
    "    global word_list\n",
    "    global Word_Documentno\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    documentP = []\n",
    "    documentsP1 = []\n",
    "    documentsP2 = []\n",
    "    documents_togetherP = []\n",
    "    \n",
    "    word1_use= porter_stemmer.stem(word1)\n",
    "    documentsP1 = Word_Documentno[word_list.index(word1_use)]\n",
    "        \n",
    "    word2_use= porter_stemmer.stem(word2)\n",
    "    documentsP2 = Word_Documentno[word_list.index(word2_use)]\n",
    "    documents_togetherP = list(set(documentsP1)|set(documentsP2))\n",
    "    \n",
    "    if len(documents_togetherP)<1:\n",
    "        documentP[0] = ['There no document include this phrase.']\n",
    "    else:\n",
    "        for number in documents_togetherP:\n",
    "            list1 = [i for i,x in enumerate(dictionary[number]) if x ==word1_use]\n",
    "            list2 = [y for y,x in enumerate(dictionary[number]) if x ==word2_use]\n",
    "            for i in list1:\n",
    "                for y in list2:\n",
    "                    if abs(i-y)<=proximity_number:\n",
    "                        documentP.append(number)\n",
    "    \n",
    "    flag = 1\n",
    "    score = [0]\n",
    "    documentP = list(set(documentP))\n",
    "    documentP = [int(x) for x in documentP]\n",
    "    documentP.sort()\n",
    "    documentP = [str(x) for x in documentP]\n",
    "    return flag, documentP, score\n",
    "\n",
    "\n",
    "\n",
    "def get_word_rank_score(wordR):\n",
    "    #score = (1+lg(tf))*lg(5002/df) (N=5002)\n",
    "    global word_list\n",
    "    global worddf\n",
    "    global Word_Documentno\n",
    "    global dictionary\n",
    "    \n",
    "    scoreR = []\n",
    "    log = math.log\n",
    "    \n",
    "    word_document_number = Word_Documentno[word_list.index(wordR)]\n",
    "    Df = float(worddf[word_list.index(wordR)])\n",
    "    for i in word_document_number:   \n",
    "        Tf = len([y for y,x in enumerate(dictionary[i]) if x == wordR])\n",
    "        Score = (1+log(Tf,10))*log((5002.0/Df),10)\n",
    "        Score = format(Score, '.4f')\n",
    "        Score = float(Score)\n",
    "        scoreR.append(Score)\n",
    "    return word_document_number,scoreR\n",
    "    \n",
    "\n",
    "def Rank(query):\n",
    "    global stop_words\n",
    "    \n",
    "    porter_stemmer = PorterStemmer()\n",
    "    Word_use = []\n",
    "    Wdocument_number = []\n",
    "    WScore = []\n",
    "    Dic_words_score = {}\n",
    "    Rdocument = []\n",
    "    Rscore = []\n",
    "    \n",
    "\n",
    "    punctuation_string = string.punctuation\n",
    "    for i in query:\n",
    "        if i in string.punctuation:\n",
    "            query = query.replace(i,\" \")\n",
    "    \n",
    "    q = query.split()\n",
    "    for qu in q:\n",
    "        if qu not in stop_words:\n",
    "            qu = porter_stemmer.stem(qu)\n",
    "            Word_use.append(qu)\n",
    "    \n",
    "    for w in Word_use:\n",
    "        Wdocument_number, WScore = get_word_rank_score(w)\n",
    "        a = len(Wdocument_number)\n",
    "        for y in range(0,a):\n",
    "            if Wdocument_number[y] not in Dic_words_score:\n",
    "                Dic_words_score[Wdocument_number[y]] = float(WScore[y])\n",
    "            else:\n",
    "                Dic_words_score[Wdocument_number[y]] = float(Dic_words_score[Wdocument_number[y]])+float(WScore[y])\n",
    "    \n",
    "    Dic_words_score = sorted(Dic_words_score.items(), key=lambda d: d[1], reverse=True)\n",
    "    for term in Dic_words_score:\n",
    "        Rdocument.append(term[0])\n",
    "        Rscore.append(term[1])\n",
    "        \n",
    "    flag = 0\n",
    "    return flag, Rdocument, Rscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QueryType(query):\n",
    "    \n",
    "    q = re.split(\" +(AND NOT|AND|OR) +\", query)\n",
    "    Q=query.split()\n",
    "\n",
    "    if len(q) > 1:\n",
    "        word1 = q[0]\n",
    "        operator = q[1]\n",
    "        word2 = q[2]\n",
    "        return BooleanSearch(word1, operator, word2)\n",
    "\n",
    "    else:\n",
    "        if '\"' in query:\n",
    "            return PhraseSearch(query, prox=1)\n",
    "        elif '#' in query:\n",
    "            word1, word2, proximity_number = get_proxomity_query_elements(query)\n",
    "            return Proximitysearch(word1, word2, proximity_number)\n",
    "        else:\n",
    "            if len(Q) == 1:\n",
    "                return Search(query)\n",
    "            else:\n",
    "                return Rank(query)\n",
    "\n",
    "                \n",
    "def excuteQueries(querypath, outputpath):\n",
    "    \n",
    "    with open(querypath, 'r') as f:\n",
    "        queries = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    with open(outputpath, 'w+') as f:\n",
    "        for line in queries:\n",
    "            if line[1] != ' ':\n",
    "                # number of query>=10\n",
    "                query_number = line[0:2]\n",
    "                query = line[3:]\n",
    "            else:\n",
    "                query_number = line[0]\n",
    "                query = line[2:]\n",
    "            \n",
    "            #query_result = [flag, document id, score]\n",
    "            #when flag=1, it means query is excuted by Boolean\\ Phrase\\ Proximity\n",
    "            #others, query is excuted by Ranked IR\n",
    "            flag, document, score = QueryType(query)\n",
    "            if flag == 1:\n",
    "                for n in document:\n",
    "                    f.write(\"{0},{1}\\n\".format(query_number, n))\n",
    "            elif flag == 0:\n",
    "                for i in range(0,150):\n",
    "                    f.write(\"{0}, {1}, {2:.4f}\\n\".format(query_number, document[i],score[i]))\n",
    "            else:\n",
    "                f.write(\"{0}:{1}\\n\".formate(query_number, 'wrong search'))\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    word_list = []\n",
    "    stop_words = []\n",
    "    dictionary = {}\n",
    "    Word_Documentno = []\n",
    "    worddf = []\n",
    "    \n",
    "    txt_path='D://ATFC//TTDS//Sem1//CW1//CW1collection//trec.5000.xml'\n",
    "    stop_word_path = 'D://ATFC//TTDS//Sem1//CW1//englishST.txt'\n",
    "    changeXML(txt_path)\n",
    "    tokenisation()\n",
    "    stopwordANDstemming(stop_word_path)\n",
    "    index()\n",
    "    \n",
    "    query_path='D://ATFC//TTDS//Sem1//CW1//CW1collection//queries.ranked.txt'\n",
    "    outputpath='D://ATFC//TTDS//Sem1//CW1//CW1 up//queries_rank_answer.txt'\n",
    "    excuteQueries(query_path,outputpath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"*\"*80)\n",
    "    print(\"\\nHello! Here is a clumsy local search engine which 'hope' you are nice day! \")\n",
    "\n",
    "    txt_path = input(\"\\nPlease enter the path to the Txt you want to use to build the index\\nPath: \")\n",
    "    while not path.exists(collection_path):\n",
    "            print(\"\\nOhoh...File doesn't exist, please try again!\\n\")\n",
    "            txt_path = (\"\\nPlease enter the path to the Txt you want to use to build the index\\nPath: \")\n",
    "\n",
    "    print(\"\\nInitializing...\")\n",
    "    changeXML(txt_path)\n",
    "    tokenisation()\n",
    "    wl = stopwordANDstemming()\n",
    "    print(\"\\nI'm Writing index...\")\n",
    "    index(wl)\n",
    "    print(\"\\nNow an Invert Index is ready in your root!\")\n",
    "\n",
    "    loop = True\n",
    "        while loop:\n",
    "            querypath = input(\"\\nPlease enter the query file path:\\nPath: \")\n",
    "            while not path.exists(querypath):\n",
    "                 print(\"\\nFile doesn't exist, try again!\\n\")\n",
    "                outputpath = input(\"\\nPlease enter the output file path:\\nPath: \")\n",
    "            outputpath = input(\"\\nPlease enter the output file path:\\nPath: \")\n",
    "\n",
    "            print(\"\\nI'm Searching...\")\n",
    "            excuteQueries(querypath, outputpath)\n",
    "            print(\"\\nThe result is saved in your root!\")\n",
    "\n",
    "            q = input(\"\\n\\n\\nDo you want to perform another search? [Y]/[N]\\n\")\n",
    "            while q.lower() not in ['y', 'n']:\n",
    "                print(\"\\nOhoh...It's not a valid option, pleace try again!\\n\")\n",
    "                q = input(\"\\nDo you want to perform another search? [Y]/[N]\\n\")\n",
    "            if q.lower() == 'y':\n",
    "                loop = True\n",
    "            elif q.lower() == 'n':\n",
    "                print(\"*\" * 80)\n",
    "                print(\"\\t\\t\\tThank you very much! Again~Have a nice day!(*^▽^*)\")\n",
    "                print(\"*\" * 80)\n",
    "                loop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float\n"
     ]
    }
   ],
   "source": [
    "Score = 2.365229842\n",
    "Score = format(Score, '.4f')\n",
    "Score = float(Score)\n",
    "if isinstance(Score,str) == True:\n",
    "    print('str')\n",
    "elif isinstance(Score,float) == True:\n",
    "    print('float')\n",
    "else:\n",
    "    print('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
